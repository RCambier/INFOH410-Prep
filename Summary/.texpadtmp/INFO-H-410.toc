\contentsline {chapter}{\numberline {1}Introduction}{7}{chapter.1}
\contentsline {section}{\numberline {1.1}Old-fashioned AI}{7}{section.1.1}
\contentsline {paragraph}{Intelligence}{7}{section*.3}
\contentsline {paragraph}{The inferential engine}{7}{section*.4}
\contentsline {paragraph}{The failures}{7}{section*.5}
\contentsline {section}{\numberline {1.2}Today's AI}{8}{section.1.2}
\contentsline {paragraph}{Summary}{8}{section*.6}
\contentsline {chapter}{\numberline {2}State space search}{9}{chapter.2}
\contentsline {section}{\numberline {2.1}Exhaustive search algorithms}{9}{section.2.1}
\contentsline {paragraph}{Choice between algorithms}{10}{section*.7}
\contentsline {paragraph}{Extensions to basic algorithm}{10}{section*.8}
\contentsline {section}{\numberline {2.2}Heuristic search algorithms}{10}{section.2.2}
\contentsline {paragraph}{Summary}{11}{section*.9}
\contentsline {chapter}{\numberline {3}Concept learning}{13}{chapter.3}
\contentsline {section}{\numberline {3.1}The inductive learning hypothesis}{13}{section.3.1}
\contentsline {paragraph}{An example problem}{13}{section*.10}
\contentsline {section}{\numberline {3.2}Version spaces}{14}{section.3.2}
\contentsline {paragraph}{Representing version spaces}{14}{section*.11}
\contentsline {section}{\numberline {3.3}Bias}{14}{section.3.3}
\contentsline {paragraph}{An unbiased learner}{15}{section*.12}
\contentsline {paragraph}{Inductive bias}{15}{section*.13}
\contentsline {paragraph}{Summary Points}{15}{section*.15}
\contentsline {chapter}{\numberline {4}Decisions trees}{17}{chapter.4}
\contentsline {section}{\numberline {4.1}Decision tree representation}{17}{section.4.1}
\contentsline {section}{\numberline {4.2}Entropy, information gain}{18}{section.4.2}
\contentsline {paragraph}{Entropy}{18}{section*.16}
\contentsline {paragraph}{The information gain}{18}{section*.17}
\contentsline {section}{\numberline {4.3}ID3 learning algorithm}{18}{section.4.3}
\contentsline {paragraph}{Properties of ID3}{18}{section*.18}
\contentsline {paragraph}{Inductive bias in ID3}{19}{section*.19}
\contentsline {paragraph}{Attributes with many values}{19}{section*.20}
\contentsline {paragraph}{Attributes with Costs}{19}{section*.21}
\contentsline {paragraph}{Unknown Attribute Values}{19}{section*.22}
\contentsline {section}{\numberline {4.4}Overfitting}{20}{section.4.4}
\contentsline {paragraph}{Occam's razor}{20}{section*.23}
\contentsline {paragraph}{How to avoid overfitting}{20}{section*.24}
\contentsline {paragraph}{Reduced-error pruning}{20}{section*.25}
\contentsline {chapter}{\numberline {5}Neural networks}{23}{chapter.5}
\contentsline {section}{\numberline {5.1}Perceptron}{23}{section.5.1}
\contentsline {paragraph}{Associated learning}{23}{section*.27}
\contentsline {paragraph}{Gradient descent}{24}{section*.30}
\contentsline {paragraph}{Limitations of the perceptron}{24}{section*.31}
\contentsline {section}{\numberline {5.2}Associative memories}{25}{section.5.2}
\contentsline {paragraph}{Hopfield networks}{25}{section*.33}
\contentsline {section}{\numberline {5.3}Multilayer perceptron}{26}{section.5.3}
\contentsline {chapter}{\numberline {6}Clustering}{29}{chapter.6}
\contentsline {paragraph}{Clustering instances}{29}{section*.37}
\contentsline {paragraph}{Requirements for clustering}{29}{section*.38}
\contentsline {paragraph}{Types of clustering methods}{30}{section*.39}
\contentsline {paragraph}{Similarity measure}{30}{section*.40}
\contentsline {paragraph}{Distance measure}{31}{section*.41}
\contentsline {section}{\numberline {6.1}K-Means clustering}{31}{section.6.1}
\contentsline {paragraph}{Cluster mean}{31}{section*.42}
\contentsline {paragraph}{Drawbacks of k-means clustering}{31}{section*.43}
\contentsline {section}{\numberline {6.2}Similarity-based clustering}{32}{section.6.2}
\contentsline {paragraph}{Similarity between instances}{32}{section*.44}
\contentsline {section}{\numberline {6.3}Nearest neighbor clustering}{32}{section.6.3}
\contentsline {paragraph}{Euclidean vs Manhattan distance}{33}{section*.45}
\contentsline {section}{\numberline {6.4}Ensemble clustering}{33}{section.6.4}
\contentsline {section}{\numberline {6.5}Subspace clustering}{33}{section.6.5}
\contentsline {chapter}{\numberline {7}Evaluation of hypothesis}{35}{chapter.7}
\contentsline {section}{\numberline {7.1}Two definitions of the error}{35}{section.7.1}
\contentsline {section}{\numberline {7.2}Estimators}{35}{section.7.2}
\contentsline {paragraph}{Problems estimating error}{35}{section*.46}
\contentsline {section}{\numberline {7.3}Confidence intervals for observed hypothesis error}{35}{section.7.3}
\contentsline {section}{\numberline {7.4}Binomial distribution, normal distribution and central limit theorem}{36}{section.7.4}
\contentsline {section}{\numberline {7.5}Paired $t$ tests}{36}{section.7.5}
\contentsline {section}{\numberline {7.6}Comparing learning methods}{36}{section.7.6}
\contentsline {chapter}{\numberline {8}Data, text and graph mining}{37}{chapter.8}
\contentsline {section}{\numberline {8.1}Data warehouses}{37}{section.8.1}
\contentsline {paragraph}{Preparing the data}{37}{section*.47}
\contentsline {paragraph}{The main techniques of data mining}{37}{section*.48}
\contentsline {section}{\numberline {8.2}Understanding and predicting data}{37}{section.8.2}
\contentsline {section}{\numberline {8.3}Data mining techniques}{38}{section.8.3}
\contentsline {chapter}{\numberline {9}Metaheuristics: genetic algorithms}{39}{chapter.9}
\contentsline {chapter}{\numberline {10}Reinforcement Learning}{41}{chapter.10}
\contentsline {chapter}{\numberline {11}Recommender Systems}{43}{chapter.11}
